{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Machine Translation with TensorFlow\n",
    "\n",
    "[The TensorFlow team recently released a tutorial](https://github.com/tensorflow/nmt) on [neural machine translation](https://en.wikipedia.org/wiki/Neural_machine_translation). Their tutorial shows off some of the functionality in the [TensorFlow seq2seq library](https://www.tensorflow.org/api_guides/python/contrib.seq2seq).\n",
    "\n",
    "The code presented in the tutorial is \"lightweight, high-quality, production-ready, and incorporated with the latest research ideas.\" With a pitch like that, how could we not be interested?\n",
    "\n",
    "This notebook will show you how to work with that model in Datalab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting access to the code\n",
    "\n",
    "To begin with, we need to get their code from the [tensorflow/nmt](https://github.com/tensorflow/nmt) repository to the persistent disk attached to the [GCE instance](https://cloud.google.com/compute/docs/instances/) hosting this notebook.\n",
    "\n",
    "Fortunately, any functionality that's available in a Jupyter notebooks is also available in Datalab. That means that we can access a shell on the instance through the notebook using the `!` symbol:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us clone [tensorflow/nmt](https://github.com/tensorflow/nmt) into the directory specified as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TFNMT_DIR = '/tmp/tf-nmt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into '/tmp/tf-nmt'...\n",
      "remote: Counting objects: 694, done.\u001b[K\n",
      "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
      "remote: Total 694 (delta 9), reused 7 (delta 3), pack-reused 671\u001b[K\n",
      "Receiving objects: 100% (694/694), 940.15 KiB | 2.71 MiB/s, done.\n",
      "Resolving deltas: 100% (458/458), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tensorflow/nmt $TFNMT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run the code in that tutorial as-is from this notebook, we have to change the working directory to `tf-nmt`. This is done as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Running the translation models\n",
    "\n",
    "Let us being with a modest goal: to get their pre-defined models working in this environment.\n",
    "\n",
    "Once we have managed to do so, we can start thinking about messing with the guts of their models and defining our own.\n",
    "\n",
    "We will do the following things:\n",
    "\n",
    "1. Obtain training and test data\n",
    "\n",
    "2. Train an inattentive model defined by tensorflow/nmt\n",
    "\n",
    "3. Use the model we trained to perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data\n",
    "\n",
    "Fortunately, the tutorial authors have provided a script we can use to download the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/sh\r\n",
      "# Download small-scale IWSLT15 Vietnames to English translation data for NMT\r\n",
      "# model training.\r\n",
      "#\r\n",
      "# Usage:\r\n",
      "#   ./download_iwslt15.sh path-to-output-dir\r\n",
      "#\r\n",
      "# If output directory is not specified, \"./iwslt15\" will be used as the default\r\n",
      "# output directory.\r\n",
      "OUT_DIR=\"${1:-iwslt15}\"\r\n",
      "SITE_PREFIX=\"https://nlp.stanford.edu/projects/nmt/data\"\r\n",
      "\r\n",
      "mkdir -v -p $OUT_DIR\r\n",
      "\r\n",
      "# Download iwslt15 small dataset from standford website.\r\n",
      "echo \"Download training dataset train.en and train.vi.\"\r\n",
      "curl -o \"$OUT_DIR/train.en\" \"$SITE_PREFIX/iwslt15.en-vi/train.en\"\r\n",
      "curl -o \"$OUT_DIR/train.vi\" \"$SITE_PREFIX/iwslt15.en-vi/train.vi\"\r\n",
      "\r\n",
      "echo \"Download dev dataset tst2012.en and tst2012.vi.\"\r\n",
      "curl -o \"$OUT_DIR/tst2012.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.en\"\r\n",
      "curl -o \"$OUT_DIR/tst2012.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2012.vi\"\r\n",
      "\r\n",
      "echo \"Download test dataset tst2013.en and tst2013.vi.\"\r\n",
      "curl -o \"$OUT_DIR/tst2013.en\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.en\"\r\n",
      "curl -o \"$OUT_DIR/tst2013.vi\" \"$SITE_PREFIX/iwslt15.en-vi/tst2013.vi\"\r\n",
      "\r\n",
      "echo \"Download vocab file vocab.en and vocab.vi.\"\r\n",
      "curl -o \"$OUT_DIR/vocab.en\" \"$SITE_PREFIX/iwslt15.en-vi/vocab.en\"\r\n",
      "curl -o \"$OUT_DIR/vocab.vi\" \"$SITE_PREFIX/iwslt15.en-vi/vocab.vi\"\r\n"
     ]
    }
   ],
   "source": [
    "!cat $TFNMT_DIR/nmt/scripts/download_iwslt15.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a directory in which to store this data and then use the script to download it there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '{}/data'.format(TFNMT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: created directory ‘/tmp/tf-nmt/data’\n",
      "Download training dataset train.en and train.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 12.9M  100 12.9M    0     0  35.1M      0 --:--:-- --:--:-- --:--:-- 35.2M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 17.2M  100 1   0    0     0      0      0 --:--:-- --:--:-- --:--:--     07.2M    0     0  53.8M      0 --:--:-- --:--:-- --:--:-- 54.0M\n",
      "Download dev dataset tst2012.en and tst2012.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0  2851k      0 --:--:-- --:--:-- --:--:-- 2914k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  183k  100  183k    0     0  3576k      0 --:--:-- --:--:-- --:--:-- 3607k\n",
      "Download test dataset tst2013.en and tst2013.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  129k  100  129k    0     0  2801k      0 --:--:-- --:--:-- --:--:-- 2807k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  179k  100  179k    0     0  3689k      0 --:--:-- --:--:-- --:--:-- 3740k\n",
      "Download vocab file vocab.en and vocab.vi.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  136k  100  136k    0     0  2429k      0 --:--:-- --:--:-- --:--:-- 2436k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 46767  100 46767    0     0  1292k      0 --:--:-- --:--:-- --:--:-- 1304k\n"
     ]
    }
   ],
   "source": [
    "!$TFNMT_DIR/nmt/scripts/download_iwslt15.sh $DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.en  tst2012.en  tst2013.en  vocab.en\r\n",
      "train.vi  tst2012.vi  tst2013.vi  vocab.vi\r\n"
     ]
    }
   ],
   "source": [
    "!ls $DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the data we need to continue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We begin by designating a directory into which TensorFlow can store the model checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = '{}/model'.format(TFNMT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the training job as indicated in the NMT tutorial, with only a few modifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tf-nmt\n"
     ]
    }
   ],
   "source": [
    "cd $TFNMT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Job id 0\n",
      "# hparams:\n",
      "  src=vi\n",
      "  tgt=en\n",
      "  train_prefix=/tmp/tf-nmt/data/train\n",
      "  dev_prefix=/tmp/tf-nmt/data/tst2012\n",
      "  test_prefix=/tmp/tf-nmt/data/tst2013\n",
      "  out_dir=/tmp/tf-nmt/model\n",
      "# Vocab file /tmp/tf-nmt/data/vocab.vi exists\n",
      "# Vocab file /tmp/tf-nmt/data/vocab.en exists\n",
      "  saving hparams to /tmp/tf-nmt/model/hparams\n",
      "  saving hparams to /tmp/tf-nmt/model/best_bleu/hparams\n",
      "  attention=\n",
      "  attention_architecture=standard\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/tmp/tf-nmt/model/best_bleu\n",
      "  bpe_delimiter=None\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_factor=0.98\n",
      "  decay_steps=10000\n",
      "  dev_prefix=/tmp/tf-nmt/data/tst2012\n",
      "  dropout=0.2\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  learning_rate=1.0\n",
      "  learning_rate_warmup_factor=1.0\n",
      "  learning_rate_warmup_steps=0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_embeddings_partitions=0\n",
      "  num_gpus=1\n",
      "  num_layers=2\n",
      "  num_residual_layers=0\n",
      "  num_train_steps=12000\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/tmp/tf-nmt/model\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  source_reverse=False\n",
      "  src=vi\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=/tmp/tf-nmt/data/vocab.vi\n",
      "  src_vocab_size=7709\n",
      "  start_decay_step=0\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  test_prefix=/tmp/tf-nmt/data/tst2013\n",
      "  tgt=en\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=/tmp/tf-nmt/data/vocab.en\n",
      "  tgt_vocab_size=17191\n",
      "  time_major=True\n",
      "  train_prefix=/tmp/tf-nmt/data/train\n",
      "  unit_type=lstm\n",
      "  vocab_prefix=/tmp/tf-nmt/data/vocab\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\n",
      "    \"__main__\", fname, loader, pkg_name)\n",
      "  File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\n",
      "    exec code in run_globals\n",
      "  File \"/tmp/tf-nmt/nmt/nmt.py\", line 511, in <module>\n",
      "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n",
      "  File \"/usr/local/google/home/nkash/.virtualenvs/translation-ml-on-gcp/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n",
      "    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n",
      "  File \"/tmp/tf-nmt/nmt/nmt.py\", line 504, in main\n",
      "    run_main(FLAGS, default_hparams, train_fn, inference_fn)\n",
      "  File \"/tmp/tf-nmt/nmt/nmt.py\", line 497, in run_main\n",
      "    train_fn(hparams, target_session=target_session)\n",
      "  File \"nmt/train.py\", line 171, in train\n",
      "    train_model = model_helper.create_train_model(model_creator, hparams, scope)\n",
      "  File \"nmt/model_helper.py\", line 86, in create_train_model\n",
      "    skip_count=skip_count_placeholder)\n",
      "  File \"nmt/utils/iterator_utils.py\", line 194, in get_iterator\n",
      "    batched_dataset = src_tgt_dataset.apply(\n",
      "AttributeError: 'MapDataset' object has no attribute 'apply'\n"
     ]
    }
   ],
   "source": [
    "!python -m nmt.nmt \\\n",
    "    --src=vi --tgt=en \\\n",
    "    --vocab_prefix=$DATA_DIR/vocab  \\\n",
    "    --train_prefix=$DATA_DIR/train \\\n",
    "    --dev_prefix=$DATA_DIR/tst2012  \\\n",
    "    --test_prefix=$DATA_DIR/tst2013 \\\n",
    "    --out_dir=$MODEL_DIR \\\n",
    "    --num_train_steps=12000 \\\n",
    "    --steps_per_stats=100 \\\n",
    "    --num_layers=2 \\\n",
    "    --num_units=128 \\\n",
    "    --dropout=0.2 \\\n",
    "    --metrics=bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
